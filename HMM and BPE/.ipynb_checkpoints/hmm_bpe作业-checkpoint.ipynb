{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a7a16ed",
   "metadata": {},
   "source": [
    "## ä»»åŠ¡ä¸€ï¼šHMMæ¨¡å‹ç”¨äºä¸­æ–‡åˆ†è¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030d9a94",
   "metadata": {},
   "source": [
    "ä»»åŠ¡ä¸€è¯„åˆ†æ ‡å‡†ï¼š\n",
    "1. å…±æœ‰8å¤„TODOéœ€è¦å¡«å†™ï¼Œæ¯ä¸ªTODOè®¡1-2åˆ†ï¼Œå…±9åˆ†ï¼Œé¢„è®¡ä»£ç é‡30è¡Œï¼›\n",
    "2. å…è®¸è‡ªè¡Œä¿®æ”¹ã€ç¼–å†™ä»£ç å®Œæˆï¼Œå¯¹äºè¯¥æƒ…å†µï¼Œè¯·è¡¥å……æ³¨é‡Šä»¥ä¾¿äºè¯„åˆ†ï¼Œå¦åˆ™ç»“æœä¸æ­£ç¡®å°†å¯¼è‡´è¾ƒå¤šçš„æ‰£åˆ†ï¼›\n",
    "3. å®éªŒæŠ¥å‘Š(python)/ç”¨äºè¯´æ˜å®éªŒçš„æ–‡å­—å—(jupyter notebook)ä¸é¢å¤–è®¡åˆ†ï¼Œä½†ä¸å†™ä¼šå¯¼è‡´æ‰£åˆ†ã€‚\n",
    "\n",
    "æ³¨ï¼šæœ¬ä»»åŠ¡ä»…åœ¨çŸ­å¥å­ä¸Šè¿›è¡Œæ•ˆæœæµ‹è¯•ï¼Œå› æ­¤å¯¹æ¦‚ç‡çš„è®¡ç®—å¯ç›´æ¥è¿›è¡Œè¿ä¹˜ã€‚åœ¨å®è·µä¸­ï¼Œå¸¸å…ˆå¯¹æ¦‚ç‡å–å¯¹æ•°ï¼Œå°†è¿ä¹˜å˜ä¸ºåŠ æ³•æ¥è®¡ç®—ï¼Œä»¥é¿å…å‡ºç°æ•°å€¼æº¢å‡ºçš„æƒ…å†µã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d77db9",
   "metadata": {},
   "source": [
    "å¯¼å…¥HMMå‚æ•°ï¼Œåˆå§‹åŒ–æ‰€éœ€çš„èµ·å§‹æ¦‚ç‡çŸ©é˜µï¼Œè½¬ç§»æ¦‚ç‡çŸ©é˜µï¼Œå‘å°„æ¦‚ç‡çŸ©é˜µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b36e0db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d25beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hmm_parameters.pkl\", \"rb\") as f:\n",
    "    hmm_parameters = pickle.load(f)\n",
    "\n",
    "# éæ–­å­—ï¼ˆBï¼‰ä¸ºç¬¬0è¡Œï¼Œæ–­å­—ï¼ˆIï¼‰ä¸ºç¬¬1è¡Œ\n",
    "# å‘å°„æ¦‚ç‡çŸ©é˜µä¸­ï¼Œè¯å…¸å¤§å°ä¸º65536ï¼Œä»¥æ±‰å­—çš„ordä½œä¸ºè¡Œkey\n",
    "start_probability = hmm_parameters[\"start_prob\"]  # shape(2,)\n",
    "trans_matrix = hmm_parameters[\"trans_mat\"]  # shape(2, 2)\n",
    "emission_matrix = hmm_parameters[\"emission_mat\"]  # shape(2, 65536)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7070152",
   "metadata": {},
   "source": [
    "å®šä¹‰å¾…å¤„ç†çš„å¥å­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87219e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: å°†input_sentenceä¸­çš„xxxæ›¿æ¢ä¸ºä½ çš„å§“åï¼ˆ1åˆ†ï¼‰\n",
    "input_sentence = \"é­æ¥æ˜¯ä¸€åä¼˜ç§€çš„å­¦ç”Ÿ\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1035cbc7",
   "metadata": {},
   "source": [
    "å®ç°viterbiç®—æ³•ï¼Œå¹¶ä»¥æ­¤è¿›è¡Œä¸­æ–‡åˆ†è¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf8e168",
   "metadata": {},
   "source": [
    "å¯¹äºviterbiç®—æ³•ï¼šé¦–å…ˆå°†æ±‰å­—è½¬ä¸ºæ•°å­—è¡¨ç¤ºï¼›'dp'ç”¨æ¥å‚¨å­˜ä¸åŒä½ç½®æ¯ç§éšçŠ¶æ€ï¼ˆB/Iï¼‰ä¸‹ï¼Œåˆ°è¯¥ä½ç½®ä¸ºæ­¢çš„å¥å­çš„æ¦‚ç‡ï¼›è®¡ç®—åˆå§‹ä½ç½®çš„æ¦‚ç‡ï¼š$P(h_i)*P(v_1|h_i)$ï¼›è®¡ç®—å…¶ä½™ä½ç½®çš„æ¦‚ç‡:å¯¹è¯åºåˆ—éå†çš„æ—¶å€™ï¼Œæ¯ä¸ªè¯è¿›è¡Œä¸¤æ¬¡å¯¹tagçš„éå†ï¼Œè®¡ç®—æ¯ä¸ªä½ç½®çš„æ¦‚ç‡ï¼Œå³å¯¹vtä¸­çš„$h_i$: æ±‚$max(left*P(h_i|left_{tag})P(v_t|h_i))$ï¼›'labels'ç”¨æ¥å­˜å‚¨æ¯ä¸ªä½ç½®æœ€æœ‰å¯èƒ½çš„çŠ¶æ€ï¼šå…ˆæ‰¾åˆ°æœ€ä¼˜è·¯å¾„çš„æœ€åä¸€ä¸ªè¯çš„tagï¼Œå›æº¯ï¼Œæ‰¾åˆ°æ¯ä¸ªè¯æœ€å¯èƒ½å¯¹åº”çš„tagï¼Œæ ¹æ®ä¹‹å‰è®°å½•çš„pathå›æº¯æ‰¾å‡ºæœ€ä¼˜è·¯å¾„ä¸Šçš„æ¯ä¸ªçŠ¶æ€ï¼ˆtagï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1adac849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(sent_orig, start_prob, trans_mat, emission_mat):\n",
    "    \"\"\"\n",
    "    viterbiç®—æ³•è¿›è¡Œä¸­æ–‡åˆ†è¯\n",
    "\n",
    "    Args:\n",
    "        sent_orig: str - è¾“å…¥çš„å¥å­\n",
    "        start_prob: numpy.ndarray - èµ·å§‹æ¦‚ç‡çŸ©é˜µ\n",
    "        trans_mat: numpy.ndarray - è½¬ç§»æ¦‚ç‡çŸ©é˜µ\n",
    "        emission_mat: numpy.ndarray - å‘å°„æ¦‚ç‡çŸ©é˜µ\n",
    "\n",
    "    Return:\n",
    "        str - ä¸­æ–‡åˆ†è¯çš„ç»“æœ\n",
    "    \"\"\"\n",
    "    \n",
    "    #  å°†æ±‰å­—è½¬ä¸ºæ•°å­—è¡¨ç¤º\n",
    "    sent_ord = [ord(x) for x in sent_orig]\n",
    "    \n",
    "    # `dp`ç”¨æ¥å‚¨å­˜ä¸åŒä½ç½®æ¯ç§æ ‡æ³¨ï¼ˆB/Iï¼‰çš„æœ€å¤§æ¦‚ç‡å€¼\n",
    "    dp = np.zeros((2, len(sent_ord)), dtype=float)\n",
    "    \n",
    "    # `path`ç”¨æ¥å‚¨å­˜æœ€å¤§æ¦‚ç‡å¯¹åº”çš„ä¸Šæ­¥B/Ié€‰æ‹©\n",
    "    #  ä¾‹å¦‚ path[1][7] == 1 æ„å‘³ç€ç¬¬8ä¸ªï¼ˆä»1å¼€å§‹è®¡æ•°ï¼‰å­—ç¬¦æ ‡æ³¨Iå¯¹åº”çš„æœ€å¤§æ¦‚ç‡ï¼Œå…¶å‰ä¸€æ­¥çš„éšçŠ¶æ€ä¸º1ï¼ˆIï¼‰\n",
    "    #  ä¾‹å¦‚ path[0][5] == 1 æ„å‘³ç€ç¬¬6ä¸ªå­—ç¬¦æ ‡æ³¨Bå¯¹åº”çš„æœ€å¤§æ¦‚ç‡ï¼Œå…¶å‰ä¸€æ­¥çš„éšçŠ¶æ€ä¸º1ï¼ˆIï¼‰\n",
    "    #  ä¾‹å¦‚ path[1][1] == 0 æ„å‘³ç€ç¬¬2ä¸ªå­—ç¬¦æ ‡æ³¨Iå¯¹åº”çš„æœ€å¤§æ¦‚ç‡ï¼Œå…¶å‰ä¸€æ­¥çš„éšçŠ¶æ€ä¸º0ï¼ˆBï¼‰\n",
    "    path = np.zeros((2, len(sent_ord)), dtype=int)\n",
    "    \n",
    "    #  TODO: ç¬¬ä¸€ä¸ªä½ç½®çš„æœ€å¤§æ¦‚ç‡å€¼è®¡ç®—ï¼ˆ1åˆ†ï¼‰\n",
    "    # ç¬¬ä¸€ä¸ªä½ç½®å³ä¸ºP(hi)*P(v1|hi)\n",
    "    for i in range(2):\n",
    "        dp[i][0] = start_probability[i] * emission_mat[i][sent_ord[0]]\n",
    "    \n",
    "    #  TODO: å…¶ä½™ä½ç½®çš„æœ€å¤§æ¦‚ç‡å€¼è®¡ç®—ï¼ˆå¡«å……dpå’ŒpathçŸ©é˜µï¼‰ï¼ˆ2åˆ†ï¼‰\n",
    "    # å¯¹vtä¸­çš„hi: æ±‚max(left*P(hi|left_tag)P(vt|hi))\n",
    "    # å³è¿›è¡Œä¸¤æ¬¡å¯¹tagçš„éå†ï¼Œè®¡ç®—æ¯ä¸ªä½ç½®çš„æ¦‚ç‡åŒæ—¶è®°å½•æ¯ä¸ªçŠ¶æ€æ¥è‡ªäºå‰é¢å“ªä¸ªçŠ¶æ€(N*S^2)\n",
    "    for i, ch in enumerate(sent_ord[1:]): # å¯¹è¯åºåˆ—éå†ï¼ˆNï¼‰\n",
    "        for s in range(2):\n",
    "            (prob, last_state) = max([(dp[ls, i] * trans_mat[ls][s] * emission_mat[s][ch] ,ls)  for ls in range(2)]) # åŠ¨æ€è§„åˆ’\n",
    "            dp[s][i+1] = prob # è®°å½•æ¦‚ç‡\n",
    "            path[s][i+1] = last_state # è®°å½•è·¯å¾„\n",
    "    \n",
    "    #  `labels`ç”¨æ¥å‚¨å­˜æ¯ä¸ªä½ç½®æœ€æœ‰å¯èƒ½çš„éšçŠ¶æ€\n",
    "    labels = [0 for _ in range(len(sent_ord))]\n",
    "    \n",
    "    #  TODOï¼šè®¡ç®—labelsæ¯ä¸ªä½ç½®ä¸Šçš„å€¼ï¼ˆå¡«å……labelsçŸ©é˜µï¼‰ï¼ˆ1åˆ†ï¼‰\n",
    "    (end_prob, state) = max([(dp[s][len(sent_ord)-1], s) for s in range(2)]) # æ‰¾åˆ°æœ€ä¼˜è·¯å¾„çš„æœ€åä¸€ä¸ªè¯çš„tag\n",
    "    labels[len(sent_ord)-1] = state\n",
    "    for i in range(len(sent_ord) - 1, 0, -1): # å›æº¯ï¼Œæ‰¾åˆ°æ¯ä¸ªè¯æœ€å¯èƒ½å¯¹åº”çš„tag\n",
    "        state = path[state][i] #æ ¹æ®ä¹‹å‰è®°å½•çš„pathå›æº¯æ‰¾å‡ºæœ€ä¼˜è·¯å¾„ä¸Šçš„æ¯ä¸ªçŠ¶æ€ï¼ˆtagï¼‰\n",
    "        labels[i-1] = state\n",
    "    \n",
    "    #  æ ¹æ®lalelsç”Ÿæˆåˆ‡åˆ†å¥½çš„å­—ç¬¦ä¸²\n",
    "    sent_split = []\n",
    "    for idx, label in enumerate(labels):\n",
    "        if label == 1:\n",
    "            sent_split += [sent_ord[idx], ord(\"/\")]\n",
    "        else:\n",
    "            sent_split += [sent_ord[idx]]\n",
    "    sent_split_str = \"\".join([chr(x) for x in sent_split])\n",
    "\n",
    "    return sent_split_str\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d795414b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "viterbiç®—æ³•åˆ†è¯ç»“æœï¼š é­æ¥/æ˜¯/ä¸€å/ä¼˜ç§€/çš„/å­¦ç”Ÿ/\n"
     ]
    }
   ],
   "source": [
    "print(\"viterbiç®—æ³•åˆ†è¯ç»“æœï¼š\", viterbi(input_sentence, start_probability, trans_matrix, emission_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399b8d1a",
   "metadata": {},
   "source": [
    "**viterbiç®—æ³•åˆ†è¯ç»“æœï¼š é­æ¥/æ˜¯/ä¸€å/ä¼˜ç§€/çš„/å­¦ç”Ÿ/**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fcafdb",
   "metadata": {},
   "source": [
    "å®ç°å‰å‘ç®—æ³•ï¼Œè®¡ç®—è¯¥å¥å­çš„æ¦‚ç‡å€¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81463a8b",
   "metadata": {},
   "source": [
    "å¯¹äºå‰å‘ç®—æ³•ï¼šé¦–å…ˆå°†æ±‰å­—è½¬ä¸ºæ•°å­—è¡¨ç¤ºï¼›'dp'ç”¨æ¥å‚¨å­˜ä¸åŒä½ç½®æ¯ç§éšçŠ¶æ€ï¼ˆB/Iï¼‰ä¸‹ï¼Œåˆ°è¯¥ä½ç½®ä¸ºæ­¢çš„å¥å­çš„æ¦‚ç‡ï¼›ä¸viterbiç®—æ³•ä¸€æ ·è®¡ç®—åˆå§‹ä½ç½®çš„æ¦‚ç‡ï¼›è®¡ç®—å…¶ä½™ä½ç½®çš„æ¦‚ç‡æ—¶ï¼Œæ€è·¯ä¸viterbiç®—æ³•ä¸€æ ·ï¼ˆå¯¹è¯åºåˆ—éå†çš„æ—¶å€™ï¼Œæ¯ä¸ªè¯è¿›è¡Œä¸¤æ¬¡å¯¹tagçš„éå†ï¼Œè®¡ç®—æ¯ä¸ªä½ç½®çš„æ¦‚ç‡ï¼‰ï¼Œåªæ˜¯å°†maxæ”¹ä¸ºsumï¼Œå¹¶ä¸”ä¸ç”¨è®°å½•æ¯ä¸€æ­¥çš„çŠ¶æ€ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf6796a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prob_by_forward(sent_orig, start_prob, trans_mat, emission_mat):\n",
    "    \"\"\"\n",
    "    å‰å‘ç®—æ³•ï¼Œè®¡ç®—è¾“å…¥ä¸­æ–‡å¥å­çš„æ¦‚ç‡å€¼\n",
    "\n",
    "    Args:\n",
    "        sent_orig: str - è¾“å…¥çš„å¥å­\n",
    "        start_prob: numpy.ndarray - èµ·å§‹æ¦‚ç‡çŸ©é˜µ\n",
    "        trans_mat: numpy.ndarray - è½¬ç§»æ¦‚ç‡çŸ©é˜µ\n",
    "        emission_mat: numpy.ndarray - å‘å°„æ¦‚ç‡çŸ©é˜µ\n",
    "\n",
    "    Return:\n",
    "        float - æ¦‚ç‡å€¼\n",
    "    \"\"\"\n",
    "    \n",
    "    #  å°†æ±‰å­—è½¬ä¸ºæ•°å­—è¡¨ç¤º\n",
    "    sent_ord = [ord(x) for x in sent_orig]\n",
    "\n",
    "    # `dp`ç”¨æ¥å‚¨å­˜ä¸åŒä½ç½®æ¯ç§éšçŠ¶æ€ï¼ˆB/Iï¼‰ä¸‹ï¼Œåˆ°è¯¥ä½ç½®ä¸ºæ­¢çš„å¥å­çš„æ¦‚ç‡\n",
    "    dp = np.zeros((2, len(sent_ord)), dtype=float)\n",
    "\n",
    "    # TODO: åˆå§‹ä½ç½®æ¦‚ç‡çš„è®¡ç®—ï¼ˆ1åˆ†ï¼‰\n",
    "    # ä¸viterbiä¸€æ ·\n",
    "    for i in range(2):\n",
    "        dp[i][0] = start_probability[i] * emission_mat[i][sent_ord[0]]\n",
    "    \n",
    "    # TODO: å…ˆè®¡ç®—å…¶ä½™ä½ç½®çš„æ¦‚ç‡ï¼ˆå¡«å……dpçŸ©é˜µï¼‰ï¼Œç„¶åreturnæ¦‚ç‡å€¼ï¼ˆ1åˆ†ï¼‰\n",
    "    # æ€è·¯ä¸viterbiä¸€æ ·ï¼Œåªæ˜¯maxæ”¹æˆsumï¼Œå¹¶ä¸”ä¸ç”¨è®°å½•æ¯ä¸€æ­¥çš„çŠ¶æ€\n",
    "    for i, ch in enumerate(sent_ord[1:]): # å¯¹è¯åºåˆ—éå†ï¼ˆNï¼‰\n",
    "        for s in range(2):\n",
    "            dp[s][i+1] = sum(dp[ls, i] * trans_mat[ls][s] * emission_mat[s][ch] for ls in range(2)) #è¿›è¡Œä¸¤æ¬¡å¯¹tagçš„éå†ï¼Œè®¡ç®—æ¯ä¸ªä½ç½®çš„æ¦‚ç‡ï¼ˆS^2ï¼‰\n",
    "\n",
    "    return sum([dp[i][len(sent_ord)-1] for i in range(2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59533cd8",
   "metadata": {},
   "source": [
    "å®ç°åå‘ç®—æ³•ï¼Œè®¡ç®—è¯¥å¥å­çš„æ¦‚ç‡å€¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eee6403",
   "metadata": {},
   "source": [
    "å¯¹äºåå‘ç®—æ³•ï¼šé¦–å…ˆå°†æ±‰å­—è½¬ä¸ºæ•°å­—è¡¨ç¤ºï¼›'dp'ç”¨æ¥å‚¨å­˜ä¸åŒä½ç½®æ¯ç§éšçŠ¶æ€ï¼ˆB/Iï¼‰ä¸‹ï¼Œåˆ°è¯¥ä½ç½®ä¸ºæ­¢çš„å¥å­çš„æ¦‚ç‡ï¼›ä¸viterbiç®—æ³•ä¸€æ ·è®¡ç®—åˆå§‹ä½ç½®çš„æ¦‚ç‡ï¼›è®¡ç®—å…¶ä½™ä½ç½®çš„æ¦‚ç‡æ—¶ï¼Œæ€è·¯ä¸å‰å‘ç®—æ³•ç›¸ä¼¼ï¼ˆå¯¹è¯åºåˆ—éå†çš„æ—¶å€™ï¼Œæ¯ä¸ªè¯è¿›è¡Œä¸¤æ¬¡å¯¹tagçš„éå†ï¼Œè®¡ç®—æ¯ä¸ªä½ç½®çš„æ¦‚ç‡ï¼‰ï¼Œä¸åŒçš„åœ°æ–¹æ˜¯ï¼šå¯¹$v_t$ä¸­çš„$h_i$: æ±‚$sum(right*P(right_{tag}|h_i))P(v_{t+1}|right_{tag}))$ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e898306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prob_by_backward(sent_orig, start_prob, trans_mat, emission_mat):\n",
    "    \"\"\"\n",
    "    åå‘ç®—æ³•ï¼Œè®¡ç®—è¾“å…¥ä¸­æ–‡å¥å­çš„æ¦‚ç‡å€¼\n",
    "\n",
    "    Args:\n",
    "        sent_orig: str - è¾“å…¥çš„å¥å­\n",
    "        start_prob: numpy.ndarray - èµ·å§‹æ¦‚ç‡çŸ©é˜µ\n",
    "        trans_mat: numpy.ndarray - è½¬ç§»æ¦‚ç‡çŸ©é˜µ\n",
    "        emission_mat: numpy.ndarray - å‘å°„æ¦‚ç‡çŸ©é˜µ\n",
    "\n",
    "    Return:\n",
    "        float - æ¦‚ç‡å€¼\n",
    "    \"\"\"\n",
    "    \n",
    "    #  å°†æ±‰å­—è½¬ä¸ºæ•°å­—è¡¨ç¤º\n",
    "    sent_ord = [ord(x) for x in sent_orig]\n",
    "\n",
    "    # `dp`ç”¨æ¥å‚¨å­˜ä¸åŒä½ç½®æ¯ç§éšçŠ¶æ€ï¼ˆB/Iï¼‰ä¸‹ï¼Œä»ç»“å°¾åˆ°è¯¥ä½ç½®ä¸ºæ­¢çš„å¥å­çš„æ¦‚ç‡\n",
    "    dp = np.zeros((2, len(sent_ord)), dtype=float)\n",
    "\n",
    "    # TODO: ç»ˆæœ«ä½ç½®æ¦‚ç‡çš„åˆå§‹åŒ–ï¼ˆ1åˆ†ï¼‰\n",
    "    # æœ€åä¸€ä¸ªè¯çš„betaè®°ä¸º1\n",
    "    n = len(sent_ord) - 1\n",
    "    for i in range(2):\n",
    "        dp[i][n] = 1\n",
    "    \n",
    "    # TODO: å…ˆè®¡ç®—å…¶ä½™ä½ç½®çš„æ¦‚ç‡ï¼ˆå¡«å……dpçŸ©é˜µï¼‰ï¼Œç„¶åreturnæ¦‚ç‡å€¼ï¼ˆ1åˆ†ï¼‰\n",
    "    # å¯¹vtä¸­çš„hi: æ±‚sum(right*P(right_tag|hi))P(vt+1|right_tag))\n",
    "    sent_ord.reverse()\n",
    "    for i, ch in enumerate(sent_ord[0:-1]): #å¯¹è¯åºåˆ—é€†åºéå†ï¼ˆNï¼‰\n",
    "        for s in range(2):\n",
    "            dp[s][n-i-1] = sum(dp[ls, n-i] * trans_mat[s][ls] * emission_mat[ls][ch] for ls in range(2)) #è¿›è¡Œä¸¤æ¬¡å¯¹tagçš„éå†ï¼Œè®¡ç®—æ¯ä¸ªä½ç½®çš„æ¦‚ç‡ï¼ˆS^2ï¼‰\n",
    "    sent_ord.reverse()\n",
    "    return sum([dp[i][0] * start_prob[i] * emission_mat[i][sent_ord[0]] for i in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b26101d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å‰å‘ç®—æ³•æ¦‚ç‡ï¼š 2.555201114823419e-29\n",
      "åå‘ç®—æ³•æ¦‚ç‡ï¼š 2.5552011148234187e-29\n"
     ]
    }
   ],
   "source": [
    "print(\"å‰å‘ç®—æ³•æ¦‚ç‡ï¼š\", compute_prob_by_forward(input_sentence, start_probability, trans_matrix, emission_matrix))\n",
    "print(\"åå‘ç®—æ³•æ¦‚ç‡ï¼š\", compute_prob_by_backward(input_sentence, start_probability, trans_matrix, emission_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3748cd",
   "metadata": {},
   "source": [
    "\n",
    "å‰å‘ç®—æ³•æ¦‚ç‡ï¼š 2.555201114823419e-29\n",
    "\n",
    "åå‘ç®—æ³•æ¦‚ç‡ï¼š 2.5552011148234187e-29\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e994be6e",
   "metadata": {},
   "source": [
    "## ä»»åŠ¡äºŒï¼šBPEç®—æ³•ç”¨äºè‹±æ–‡åˆ†è¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cc4775",
   "metadata": {},
   "source": [
    "ä»»åŠ¡äºŒè¯„åˆ†æ ‡å‡†ï¼š\n",
    "\n",
    "1. å…±æœ‰7å¤„TODOéœ€è¦å¡«å†™ï¼Œæ¯ä¸ªTODOè®¡1-2åˆ†ï¼Œå…±9åˆ†ï¼Œé¢„è®¡ä»£ç é‡50è¡Œï¼›\n",
    "2. å…è®¸è‡ªè¡Œä¿®æ”¹ã€ç¼–å†™ä»£ç å®Œæˆï¼Œå¯¹äºè¯¥æƒ…å†µï¼Œè¯·è¡¥å……æ³¨é‡Šä»¥ä¾¿äºè¯„åˆ†ï¼Œå¦åˆ™ç»“æœä¸æ­£ç¡®å°†å¯¼è‡´è¾ƒå¤šçš„æ‰£åˆ†ï¼›\n",
    "3. å®éªŒæŠ¥å‘Š(python)/ç”¨äºè¯´æ˜å®éªŒçš„æ–‡å­—å—(jupyter notebook)ä¸é¢å¤–è®¡åˆ†ï¼Œä½†ä¸å†™ä¼šå¯¼è‡´æ‰£åˆ†ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb5dbb9",
   "metadata": {},
   "source": [
    "æ„å»ºç©ºæ ¼åˆ†è¯å™¨ï¼Œå°†è¯­æ–™ä¸­çš„å¥å­ä»¥ç©ºæ ¼åˆ‡åˆ†æˆå•è¯ï¼Œç„¶åå°†å•è¯æ‹†åˆ†æˆå­—æ¯åŠ `</w>`çš„å½¢å¼ã€‚ä¾‹å¦‚`apple`å°†å˜ä¸º`a p p l e </w>`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70e10703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6c3667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_splitor_pattern = re.compile(r\"[^a-zA-Z']+|(?=')\")\n",
    "_digit_pattern = re.compile(r\"\\d+\")\n",
    "\n",
    "def white_space_tokenize(corpus):\n",
    "    \"\"\"\n",
    "    å…ˆæ­£åˆ™åŒ–ï¼ˆå­—æ¯è½¬å°å†™ã€æ•°å­—è½¬ä¸ºNã€é™¤å»æ ‡ç‚¹ç¬¦å·ï¼‰ï¼Œç„¶åä»¥ç©ºæ ¼åˆ†è¯è¯­æ–™ä¸­çš„å¥å­ï¼Œä¾‹å¦‚ï¼š\n",
    "    è¾“å…¥ corpus=[\"I am happy.\", \"I have 10 apples!\"]ï¼Œ\n",
    "    å¾—åˆ° [[\"i\", \"am\", \"happy\"], [\"i\", \"have\", \"N\", \"apples\"]]\n",
    "\n",
    "    Args:\n",
    "        corpus: List[str] - å¾…å¤„ç†çš„è¯­æ–™\n",
    "\n",
    "    Return:\n",
    "        List[List[str]] - äºŒç»´Listï¼Œå†…éƒ¨çš„Listç”±æ¯ä¸ªå¥å­çš„å•è¯stræ„æˆ\n",
    "    \"\"\"\n",
    "\n",
    "    tokeneds = [list(\n",
    "        filter(lambda tkn: len(tkn)>0, _splitor_pattern.split(_digit_pattern.sub(\"N\", stc.lower())))) for stc in corpus\n",
    "    ]\n",
    "    \n",
    "    return tokeneds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3732502a",
   "metadata": {},
   "source": [
    "ç¼–å†™ç›¸åº”å‡½æ•°æ„å»ºBPEç®—æ³•éœ€è¦ç”¨åˆ°çš„åˆå§‹çŠ¶æ€è¯å…¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b79b00",
   "metadata": {},
   "source": [
    "å¯¹æ¯ä¸ªå¥å­ä¸­æ¯ä¸ªå•è¯å•ç‹¬å¤„ç†ï¼šå°†å•è¯çš„æ¯ä¸ªå­—æ¯ä»¥ç©ºæ ¼éš”å¼€ï¼Œç»“å°¾åŠ ä¸Š<\\/w>ï¼Œä»¥æ­¤æ„å»ºå¸¦é¢‘æ•°çš„å­—å…¸ï¼ˆè¡¨ä¸­æœ‰è¯¥å•è¯åˆ™è¯é¢‘å¢åŠ ï¼Œæ²¡æœ‰åˆ™åˆ›ç«‹æ–°çš„é”®å€¼å¯¹ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bf823e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bpe_vocab(corpus):\n",
    "    \"\"\"\n",
    "    å°†è¯­æ–™è¿›è¡Œwhite_space_tokenizeå¤„ç†åï¼Œå°†å•è¯æ¯ä¸ªå­—æ¯ä»¥ç©ºæ ¼éš”å¼€ã€ç»“å°¾åŠ ä¸Š</w>åï¼Œæ„å»ºå¸¦é¢‘æ•°çš„å­—å…¸ï¼Œä¾‹å¦‚ï¼š\n",
    "    è¾“å…¥ corpus=[\"I am happy.\", \"I have 10 apples!\"]ï¼Œ\n",
    "    å¾—åˆ°\n",
    "    {\n",
    "        'i </w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "     }\n",
    "\n",
    "    Args:\n",
    "        corpus: List[str] - å¾…å¤„ç†çš„è¯­æ–™\n",
    "\n",
    "    Return:\n",
    "        Dict[str, int] - \"å•è¯åˆ†è¯çŠ¶æ€->é¢‘æ•°\"çš„è¯å…¸\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_corpus = white_space_tokenize(corpus)\n",
    "\n",
    "    bpe_vocab = dict()\n",
    "    \n",
    "    # TODO: å®Œæˆå‡½æ•°ä½“ï¼ˆ1åˆ†ï¼‰\n",
    "    # å¯¹æ¯ä¸ªå¥å­ä¸­çš„æ¯ä¸ªè¯å•ç‹¬å¤„ç†ğŸ™‚\n",
    "    for se in tokenized_corpus:\n",
    "        for word in se:\n",
    "            ch = \" \".join(word) # å°†å•è¯æ¯ä¸ªå­—æ¯ä»¥ç©ºæ ¼éš”å¼€\n",
    "            ch += ' </w>' # ç»“å°¾åŠ ä¸Š</w>\n",
    "            if ch in bpe_vocab:\n",
    "                bpe_vocab[ch] += 1 # æ„å»ºå¸¦é¢‘æ•°çš„å­—å…¸ï¼ˆè¡¨ä¸­æœ‰è¯¥å•è¯åˆ™è¯é¢‘å¢åŠ ï¼Œæ²¡æœ‰åˆ™åˆ›ç«‹æ–°çš„é”®å€¼å¯¹ï¼‰\n",
    "            else:\n",
    "                bpe_vocab[ch] = 1\n",
    "\n",
    "    return bpe_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d25245",
   "metadata": {},
   "source": [
    "ç¼–å†™æ‰€éœ€çš„å…¶ä»–å‡½æ•°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaa013e",
   "metadata": {},
   "source": [
    "éå†è¯è¡¨çš„æ¯ä¸ªè¯ã€è¯é¢‘æ—¶ï¼šå…ˆå°†æ¯ä¸ªä»¥ç©ºæ ¼éš”å¼€çš„å­—ç¬¦å­˜å…¥åˆ—è¡¨ï¼Œå¯¹æ¯ä¸ªè¯è¿›è¡Œå­—æ¯ä¸¤ä¸¤ç»„åˆï¼Œå¡«å…¥æ–°çš„bigramè¯è¡¨ï¼ˆè¡¨ä¸­æœ‰bigramåˆ™è¯é¢‘å¢åŠ ï¼Œæ²¡æœ‰åˆ™åˆ›ç«‹æ–°çš„é”®å€¼å¯¹ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "087d11e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_freq(bpe_vocab):\n",
    "    \"\"\"\n",
    "    ç»Ÿè®¡\"å•è¯åˆ†è¯çŠ¶æ€->é¢‘æ•°\"çš„è¯å…¸ä¸­ï¼Œå„bigramçš„é¢‘æ¬¡ï¼ˆå‡è®¾è¯¥è¯å…¸ä¸­ï¼Œå„ä¸ªunigramä»¥ç©ºæ ¼é—´éš”ï¼‰ï¼Œä¾‹å¦‚ï¼š\n",
    "    è¾“å…¥ bpe_vocab=\n",
    "    {\n",
    "        'i </w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "    å¾—åˆ°\n",
    "    {\n",
    "        ('i', '</w>'): 2,\n",
    "        ('a', 'm'): 1,\n",
    "        ('m', '</w>'): 1,\n",
    "        ('h', 'a'): 2,\n",
    "        ('a', 'p'): 2,\n",
    "        ('p', 'p'): 2,\n",
    "        ('p', 'y'): 1,\n",
    "        ('y', '</w>'): 1,\n",
    "        ('a', 'v'): 1,\n",
    "        ('v', 'e'): 1,\n",
    "        ('e', '</w>'): 1,\n",
    "        ('N', '</w>'): 1,\n",
    "        ('p', 'l'): 1,\n",
    "        ('l', 'e'): 1,\n",
    "        ('e', 's'): 1,\n",
    "        ('s', '</w>'): 1\n",
    "    }\n",
    "\n",
    "    Args:\n",
    "        bpe_vocab: Dict[str, int] - \"å•è¯åˆ†è¯çŠ¶æ€->é¢‘æ•°\"çš„è¯å…¸\n",
    "\n",
    "    Return:\n",
    "        Dict[Tuple(str, str), int] - \"bigram->é¢‘æ•°\"çš„è¯å…¸\n",
    "    \"\"\"\n",
    "\n",
    "    bigram_freq = dict()\n",
    "    \n",
    "    # TODO: å®Œæˆå‡½æ•°ä½“ï¼ˆ1åˆ†ï¼‰\n",
    "    for ch, fr in bpe_vocab.items(): #éå†è¯è¡¨çš„æ¯ä¸ªè¯ã€è¯é¢‘\n",
    "        word = ch.split() #å…ˆå°†æ¯ä¸ªä»¥ç©ºæ ¼éš”å¼€çš„å­—ç¬¦å­˜å…¥åˆ—è¡¨\n",
    "        for i in range(1, len(word)):\n",
    "            if tuple(word[i-1:i+1]) in bigram_freq: #å¯¹æ¯ä¸ªè¯è¿›è¡Œå­—æ¯ä¸¤ä¸¤ç»„åˆ\n",
    "                bigram_freq[tuple(word[i - 1:i + 1])] += fr #å¡«å…¥æ–°çš„bigramè¯è¡¨ï¼ˆè¡¨ä¸­æœ‰bigramåˆ™è¯é¢‘å¢åŠ ï¼Œæ²¡æœ‰åˆ™åˆ›ç«‹æ–°çš„é”®å€¼å¯¹ï¼‰\n",
    "            else:\n",
    "                bigram_freq[tuple(word[i - 1:i + 1])] = fr\n",
    "\n",
    "    return bigram_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8d5277",
   "metadata": {},
   "source": [
    "å°†bigramå…ƒç»„æ”¹ä¸ºä»¥ç©ºæ ¼ä¸ºé—´çš„å­—ç¬¦ä¸²ï¼›éå†æ—§è¯è¡¨çš„æ¯ä¸ªè¯ã€è¯é¢‘æ—¶ï¼šè‹¥bigramåŒ¹é…ä¸Šäº†æŸä¸ªè¯ï¼Œæ‰¾åˆ°bigramåœ¨è¯ä¸­çš„èµ·å§‹ä½å’Œç»“æŸä½ï¼Œåˆå¹¶bigramï¼ˆå³å»æ‰å¯¹åº”çš„ç›¸é‚»unigramä¹‹é—´çš„ç©ºæ ¼ï¼‰ï¼›è‹¥æœªåŒ¹é…ï¼Œåˆ™ç›´æ¥å½•å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba426043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_bpe_vocab_by_merging_bigram(bigram, old_bpe_vocab):\n",
    "    \"\"\"\n",
    "    åœ¨\"å•è¯åˆ†è¯çŠ¶æ€->é¢‘æ•°\"çš„è¯å…¸ä¸­ï¼Œåˆå¹¶æŒ‡å®šçš„bigramï¼ˆå³å»æ‰å¯¹åº”çš„ç›¸é‚»unigramä¹‹é—´çš„ç©ºæ ¼ï¼‰ï¼Œæœ€åè¿”å›æ–°çš„è¯å…¸ï¼Œä¾‹å¦‚ï¼š\n",
    "    è¾“å…¥ bigram=('i', '</w>')ï¼Œold_bpe_vocab=\n",
    "    {\n",
    "        'i </w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "    å¾—åˆ°\n",
    "    {\n",
    "        'i</w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "\n",
    "    Args:\n",
    "        old_bpe_vocab: Dict[str, int] - åˆå§‹\"å•è¯åˆ†è¯çŠ¶æ€->é¢‘æ•°\"çš„è¯å…¸\n",
    "\n",
    "    Return:\n",
    "        Dict[str, int] - åˆå¹¶åçš„\"å•è¯åˆ†è¯çŠ¶æ€->é¢‘æ•°\"çš„è¯å…¸\n",
    "    \"\"\"\n",
    "    \n",
    "    new_bpe_vocab = dict()\n",
    "\n",
    "    # TODO: å®Œæˆå‡½æ•°ä½“ï¼ˆ1åˆ†ï¼‰\n",
    "    word = \" \".join(bigram) #å°†bigramå…ƒç»„æ”¹ä¸ºä»¥ç©ºæ ¼ä¸ºé—´çš„å­—ç¬¦ä¸²\n",
    "    for ch, fr in old_bpe_vocab.items(): # éå†æ—§è¯è¡¨çš„æ¯ä¸ªè¯ã€è¯é¢‘\n",
    "        if word in ch: # bigramåŒ¹é…ä¸Šäº†æŸä¸ªè¯\n",
    "            lft = ch.index(word) #æ‰¾åˆ°bigramåœ¨è¯ä¸­çš„èµ·å§‹ä½\n",
    "            rht = lft + len(word) #æ‰¾åˆ°bigramåœ¨è¯ä¸­çš„ç»“æŸä½\n",
    "            new_bpe_vocab[ch[0:lft] + \"\".join(bigram) + ch[rht:]] = fr #åˆå¹¶bigramï¼ˆå³å»æ‰å¯¹åº”çš„ç›¸é‚»unigramä¹‹é—´çš„ç©ºæ ¼ï¼‰\n",
    "        else:\n",
    "            new_bpe_vocab[ch] = fr #bigramæœªåŒ¹é…ï¼Œåˆ™ç›´æ¥å½•å…¥\n",
    "    \n",
    "    return new_bpe_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdac1bf",
   "metadata": {},
   "source": [
    "å…ˆåˆ›å»ºä¸€ä¸ªå­—å…¸ä½œä¸ºä¸´æ—¶å­˜å‚¨ï¼šå°†æ¯ä¸ªä»¥ç©ºæ ¼éš”å¼€çš„å­—ç¬¦å­˜å…¥åˆ—è¡¨å†è¿›è¡Œéå†ï¼Œå¾€å­—å…¸é‡Œå­˜å…¥å­—ç¬¦ã€å­—é¢‘ï¼ˆè¡¨ä¸­æœ‰è¯¥å­—ç¬¦åˆ™å­—é¢‘å¢åŠ ï¼Œæ²¡æœ‰åˆ™åˆ›ç«‹æ–°çš„é”®å€¼å¯¹ï¼‰ã€‚å°†ç»Ÿè®¡å¥½çš„å­—å…¸è¾“å…¥åˆ—è¡¨ï¼Œå°†è¯¥åˆ—è¡¨æŒ‰ç…§åˆ†è¯é•¿åº¦æ’åºè¿”å›ï¼ˆ'\\</w>'è®¡ä¸ºä¸€ä¸ªé•¿åº¦ï¼‰ï¼Œæœ€åå†æ”¹ä¸ºé™åº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "992438a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bpe_tokens(bpe_vocab):\n",
    "    \"\"\"\n",
    "    æ ¹æ®\"å•è¯åˆ†è¯çŠ¶æ€->é¢‘æ•°\"çš„è¯å…¸ï¼Œè¿”å›æ‰€å¾—åˆ°çš„BPEåˆ†è¯åˆ—è¡¨ï¼Œå¹¶å°†è¯¥åˆ—è¡¨æŒ‰ç…§åˆ†è¯é•¿åº¦é™åºæ’åºè¿”å›ï¼Œä¾‹å¦‚ï¼š\n",
    "    è¾“å…¥ bpe_vocab=\n",
    "    {\n",
    "        'i</w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'ha pp y </w>': 1,\n",
    "        'ha v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a pp l e s </w>': 1\n",
    "    }\n",
    "    å¾—åˆ°\n",
    "    [\n",
    "        ('i</w>', 2),\n",
    "        ('ha', 2),\n",
    "        ('pp', 2),\n",
    "        ('a', 2),\n",
    "        ('m', 1),\n",
    "        ('</w>', 5),\n",
    "        ('y', 1),\n",
    "        ('v', 1),\n",
    "        ('e', 2),\n",
    "        ('N', 1),\n",
    "        ('l', 1),\n",
    "        ('s', 1)\n",
    "     ]\n",
    "\n",
    "    Args:\n",
    "        bpe_vocab: Dict[str, int] - \"å•è¯åˆ†è¯çŠ¶æ€->é¢‘æ•°\"çš„è¯å…¸\n",
    "\n",
    "    Return:\n",
    "        List[Tuple(str, int)] - BPEåˆ†è¯å’Œå¯¹åº”é¢‘æ•°ç»„æˆçš„List\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: å®Œæˆå‡½æ•°ä½“ï¼ˆ2åˆ†ï¼‰\n",
    "    bpe_token = dict() #åˆ›å»ºå­—å…¸ä¸´æ—¶å­˜å‚¨\n",
    "    for ch, fr in bpe_vocab.items():\n",
    "        word = ch.split() #å…ˆå°†æ¯ä¸ªä»¥ç©ºæ ¼éš”å¼€çš„å­—ç¬¦å­˜å…¥åˆ—è¡¨\n",
    "        for gram in word: #éå†å­—ç¬¦\n",
    "            if gram in bpe_token:\n",
    "                bpe_token[gram] += fr # å­˜å…¥å­—ç¬¦ã€å­—é¢‘ï¼ˆè¡¨ä¸­æœ‰è¯¥å­—ç¬¦åˆ™å­—é¢‘å¢åŠ ï¼Œæ²¡æœ‰åˆ™åˆ›ç«‹æ–°çš„é”®å€¼å¯¹ï¼‰\n",
    "            else:\n",
    "                bpe_token[gram] = fr\n",
    "    bpe_tokens = []\n",
    "    for ch, fr in bpe_token.items(): #å°†ç»Ÿè®¡å¥½çš„å­—å…¸è¾“å…¥åˆ—è¡¨\n",
    "        bpe_tokens.append((ch, fr))\n",
    "    # å°†è¯¥åˆ—è¡¨æŒ‰ç…§åˆ†è¯é•¿åº¦æ’åºè¿”å›ï¼ˆ'</w>'è®¡ä¸ºä¸€ä¸ªé•¿åº¦ï¼‰\n",
    "    bpe_tokens = sorted(bpe_tokens, key=lambda pair: len(pair[0]) if '</w>' not in pair[0] else len(pair[0]) - 3)\n",
    "    bpe_tokens.reverse() #æ”¹ä¸ºé™åº\n",
    "\n",
    "    return bpe_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40b79cd",
   "metadata": {},
   "source": [
    "åœ¨é€’å½’å‡½æ•°ä¸­ï¼šå…ˆç¡®å®šé€’å½’ç»“æŸæ¡ä»¶ï¼Œå³ä¸ºå‰©ä½™éƒ¨åˆ†é•¿åº¦ä¸º0å’Œå‰©ä½™éƒ¨åˆ†æ— æ³•åŒ¹é…(ç”¨\"\\<unknown>\"ä»£æ›¿)ï¼›åœ¨å‡½æ•°ä¸»ä½“éƒ¨åˆ†ï¼Œå…ˆéå†bpeåˆ†è¯åˆ—è¡¨ï¼Œè‹¥æˆåŠŸåŒ¹é…ï¼Œåˆ™æ‰¾å‡ºbpeåˆ†è¯åœ¨å•è¯ä¸­çš„èµ·å§‹ä½ä¸ç»“æŸä½ï¼Œå†å¯¹è¯¥å­ä¸²å·¦å³å‰©ä½™éƒ¨åˆ†é€’å½’åœ°è¿›è¡Œä¸‹ä¸€è½®åŒ¹é…ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c56995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_bpe_tokenize(word, bpe_tokens):\n",
    "    \"\"\"\n",
    "    æ ¹æ®æŒ‰é•¿åº¦é™åºçš„BPEåˆ†è¯åˆ—è¡¨ï¼Œå°†æ‰€ç»™å•è¯è¿›è¡ŒBPEåˆ†è¯ï¼Œæœ€åæ‰“å°ç»“æœã€‚\n",
    "    \n",
    "    æ€æƒ³æ˜¯ï¼Œå¯¹äºä¸€ä¸ªå¾…BPEåˆ†è¯çš„å•è¯ï¼ŒæŒ‰ç…§é•¿åº¦é¡ºåºä»åˆ—è¡¨ä¸­å¯»æ‰¾BPEåˆ†è¯è¿›è¡Œå­ä¸²åŒ¹é…ï¼Œ\n",
    "    è‹¥æˆåŠŸåŒ¹é…ï¼Œåˆ™å¯¹è¯¥å­ä¸²å·¦å³çš„å‰©ä½™éƒ¨åˆ†é€’å½’åœ°è¿›è¡Œä¸‹ä¸€è½®åŒ¹é…ï¼Œç›´åˆ°å‰©ä½™éƒ¨åˆ†é•¿åº¦ä¸º0ï¼Œ\n",
    "    æˆ–è€…å‰©ä½™éƒ¨åˆ†æ— æ³•åŒ¹é…ï¼ˆè¯¥éƒ¨åˆ†æ•´ä½“ç”±\"<unknown>\"ä»£æ›¿ï¼‰\n",
    "    \n",
    "    ä¾‹1ï¼š\n",
    "    è¾“å…¥ word=\"supermarket\", bpe_tokens=[\n",
    "        (\"su\", 20),\n",
    "        (\"are\", 10),\n",
    "        (\"per\", 30),\n",
    "    ]\n",
    "    æœ€ç»ˆæ‰“å° \"su per <unknown>\"\n",
    "\n",
    "    ä¾‹2ï¼š\n",
    "    è¾“å…¥ word=\"shanghai\", bpe_tokens=[\n",
    "        (\"hai\", 1),\n",
    "        (\"sh\", 1),\n",
    "        (\"an\", 1),\n",
    "        (\"</w>\", 1),\n",
    "        (\"g\", 1)\n",
    "    ]\n",
    "    æœ€ç»ˆæ‰“å° \"sh an g hai </w>\"\n",
    "\n",
    "    Args:\n",
    "        word: str - å¾…åˆ†è¯çš„å•è¯str\n",
    "        bpe_tokens: List[Tuple(str, int)] - BPEåˆ†è¯å’Œå¯¹åº”é¢‘æ•°ç»„æˆçš„List\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: è¯·å°è¯•ä½¿ç”¨é€’å½’å‡½æ•°å®šä¹‰è¯¥åˆ†è¯è¿‡ç¨‹ï¼ˆ2åˆ†ï¼‰\n",
    "    def bpe_tokenize(sub_word):\n",
    "        if len(sub_word) == 0: #é€’å½’ç»“æŸæ¡ä»¶ï¼šå‰©ä½™éƒ¨åˆ†é•¿åº¦ä¸º0\n",
    "            return \"\"\n",
    "        for i, pair in enumerate(bpe_tokens): #éå†bpeåˆ†è¯åˆ—è¡¨\n",
    "            if pair[0] in sub_word: #æˆåŠŸåŒ¹é…\n",
    "                lft = sub_word.index(pair[0]) #æ‰¾å‡ºbpeåˆ†è¯åœ¨å•è¯ä¸­çš„èµ·å§‹ä½\n",
    "                rht = lft + len(pair[0]) #æ‰¾å‡ºbpeåˆ†è¯åœ¨å•è¯ä¸­çš„ç»“æŸä½\n",
    "                return bpe_tokenize(sub_word[0:lft]) + pair[0] + \" \" + bpe_tokenize(sub_word[rht:]) #å¯¹è¯¥å­ä¸²å·¦å³çš„å‰©ä½™éƒ¨åˆ†é€’å½’åœ°è¿›è¡Œä¸‹ä¸€è½®åŒ¹é…\n",
    "            else:\n",
    "                if i == len(bpe_tokens) - 1: #é€’å½’ç»“æŸæ¡ä»¶ï¼šå‰©ä½™éƒ¨åˆ†æ— æ³•åŒ¹é…ï¼ˆè¯¥éƒ¨åˆ†æ•´ä½“ç”±\"<unknown>\"ä»£æ›¿ï¼‰\n",
    "                    return \"<unknown>\" + \" \"\n",
    "        return \"\"\n",
    "\n",
    "    res = bpe_tokenize(word+\"</w>\")\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd70402",
   "metadata": {},
   "source": [
    "å¼€å§‹è¯»å–æ•°æ®é›†å¹¶è®­ç»ƒBPEåˆ†è¯å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "215b56d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training corpus.\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/news.2007.en.shuffled.deduped.train\", encoding=\"utf-8\") as f:\n",
    "    training_corpus = list(map(lambda l: l.strip(), f.readlines()[:1000]))\n",
    "\n",
    "print(\"Loaded training corpus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5d16e9",
   "metadata": {},
   "source": [
    "è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå…ˆåˆ›å»ºbigramè¯è¡¨ï¼Œå†ä»ä¸­æ‰¾å‡ºæœ€å¸¸è§çš„ä¸€ä¸ªbigramï¼Œå°†è¯¥bigramæåˆæˆæ–°çš„tokenï¼Œæ„æˆæ–°è¯è¡¨ï¼Œå¾ªç¯300æ¬¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bccd41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_iter_num = 300\n",
    "\n",
    "training_bpe_vocab = build_bpe_vocab(training_corpus)\n",
    "for i in range(training_iter_num):\n",
    "    # TODO: å®Œæˆè®­ç»ƒå¾ªç¯å†…çš„ä»£ç é€»è¾‘ï¼ˆ2åˆ†ï¼‰\n",
    "    bigram_freq = get_bigram_freq(training_bpe_vocab) #åˆ›å»ºbigramè¯è¡¨\n",
    "    max_key = max(bigram_freq, key=bigram_freq.get) #æ‰¾åˆ°bigramè¯è¡¨ä¸­æœ€å¸¸è§çš„ä¸€ä¸ªbigram\n",
    "    training_bpe_vocab = refresh_bpe_vocab_by_merging_bigram(max_key, training_bpe_vocab) #å°†æœ€å¸¸è§çš„bigramæåˆæˆæ–°çš„tokenï¼Œæ„æˆæ–°è¯è¡¨\n",
    "\n",
    "training_bpe_tokens = get_bpe_tokens(training_bpe_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea3ddd",
   "metadata": {},
   "source": [
    "æµ‹è¯•BPEåˆ†è¯å™¨çš„åˆ†è¯æ•ˆæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0cfdb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naturallanguageprocessing çš„åˆ†è¯ç»“æœä¸ºï¼š\n",
      "n atur al lan gu age pro ce s sing</w> \n"
     ]
    }
   ],
   "source": [
    "test_word = \"naturallanguageprocessing\"\n",
    "\n",
    "print(\"naturallanguageprocessing çš„åˆ†è¯ç»“æœä¸ºï¼š\")\n",
    "print_bpe_tokenize(test_word, training_bpe_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f269fb9",
   "metadata": {},
   "source": [
    "æµ‹è¯•ç»“æœï¼š n atur al lan gu age pro ce s sing<\\/w> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
