


import re
import functools


# æ„å»ºç©ºæ ¼åˆ†è¯å™¨ï¼Œå°†è¯­æ–™ä¸­çš„å¥å­ä»¥ç©ºæ ¼åˆ‡åˆ†æˆå•è¯ï¼Œç„¶åå°†å•è¯æ‹†åˆ†æˆå­—æ¯åŠ `</w>`çš„å½¢å¼ã€‚ä¾‹å¦‚`apple`å°†å˜ä¸º`a p p l e </w>`ã€‚
_splitor_pattern = re.compile(r"[^a-zA-Z']+|(?=')")
_digit_pattern = re.compile(r"\d+")

def white_space_tokenize(corpus):
    """
    å…ˆæ­£åˆ™åŒ–ï¼ˆå­—æ¯è½¬å°å†™ã€æ•°å­—è½¬ä¸ºNã€é™¤å»æ ‡ç‚¹ç¬¦å·ï¼‰ï¼Œç„¶åä»¥ç©ºæ ¼åˆ†è¯è¯­æ–™ä¸­çš„å¥å­ï¼Œä¾‹å¦‚ï¼š
    è¾“å…¥ corpus=["I am happy.", "I have 10 apples!"]ï¼Œ
    å¾—åˆ° [["i", "am", "happy"], ["i", "have", "N", "apples"]]

    Args:
        corpus: List[str] - å¾…å¤„ç†çš„è¯­æ–™

    Return:
        List[List[str]] - äºŒç»´Listï¼Œå†…éƒ¨çš„Listç”±æ¯ä¸ªå¥å­çš„å•è¯stræ„æˆ
    """

    tokeneds = [list(
        filter(lambda tkn: len(tkn)>0, _splitor_pattern.split(_digit_pattern.sub("N", stc.lower())))) for stc in corpus
    ]
    
    return tokeneds


# ç¼–å†™ç›¸åº”å‡½æ•°æ„å»ºBPEç®—æ³•éœ€è¦ç”¨åˆ°çš„åˆå§‹çŠ¶æ€è¯å…¸
def build_bpe_vocab(corpus):
    """
    å°†è¯­æ–™è¿›è¡Œwhite_space_tokenizeå¤„ç†åï¼Œå°†å•è¯æ¯ä¸ªå­—æ¯ä»¥ç©ºæ ¼éš”å¼€ã€ç»“å°¾åŠ ä¸Š</w>åï¼Œæ„å»ºå¸¦é¢‘æ•°çš„å­—å…¸ï¼Œä¾‹å¦‚ï¼š
    è¾“å…¥ corpus=["I am happy.", "I have 10 apples!"]ï¼Œ
    å¾—åˆ°
    {
        'i </w>': 2,
        'a m </w>': 1,
        'h a p p y </w>': 1,
        'h a v e </w>': 1,
        'N </w>': 1,
        'a p p l e s </w>': 1
     }

    Args:
        corpus: List[str] - å¾…å¤„ç†çš„è¯­æ–™

    Return:
        Dict[str, int] - "å•è¯åˆ†è¯çŠ¶æ€->é¢‘æ•°"çš„è¯å…¸
    """

    tokenized_corpus = white_space_tokenize(corpus)

    bpe_vocab = dict()
    
    # TODO: å®Œæˆå‡½æ•°ä½“ï¼ˆ1åˆ†ï¼‰
    # å¯¹æ¯ä¸ªå¥å­ä¸­çš„æ¯ä¸ªè¯å•ç‹¬å¤„ç†ğŸ™‚
    for se in tokenized_corpus:
        for word in se:
            ch = " ".join(word) # å°†å•è¯æ¯ä¸ªå­—æ¯ä»¥ç©ºæ ¼éš”å¼€
            ch += ' </w>' # ç»“å°¾åŠ ä¸Š</w>
            if ch in bpe_vocab:
                bpe_vocab[ch] += 1 # æ„å»ºå¸¦é¢‘æ•°çš„å­—å…¸ï¼ˆè¡¨ä¸­æœ‰è¯¥å•è¯åˆ™è¯é¢‘å¢åŠ ï¼Œæ²¡æœ‰åˆ™åˆ›ç«‹æ–°çš„é”®å€¼å¯¹ï¼‰
            else:
                bpe_vocab[ch] = 1
    return bpe_vocab


# ç¼–å†™æ‰€éœ€çš„å…¶ä»–å‡½æ•°
def get_bigram_freq(bpe_vocab):
    """
    ç»Ÿè®¡"å•è¯åˆ†è¯çŠ¶æ€->é¢‘æ•°"çš„è¯å…¸ä¸­ï¼Œå„bigramçš„é¢‘æ¬¡ï¼ˆå‡è®¾è¯¥è¯å…¸ä¸­ï¼Œå„ä¸ªunigramä»¥ç©ºæ ¼é—´éš”ï¼‰ï¼Œä¾‹å¦‚ï¼š
    è¾“å…¥ bpe_vocab=
    {
        'i </w>': 2,
        'a m </w>': 1,
        'h a p p y </w>': 1,
        'h a v e </w>': 1,
        'N </w>': 1,
        'a p p l e s </w>': 1
    }
    å¾—åˆ°
    {
        ('i', '</w>'): 2,
        ('a', 'm'): 1,
        ('m', '</w>'): 1,
        ('h', 'a'): 2,
        ('a', 'p'): 2,
        ('p', 'p'): 2,
        ('p', 'y'): 1,
        ('y', '</w>'): 1,
        ('a', 'v'): 1,
        ('v', 'e'): 1,
        ('e', '</w>'): 1,
        ('N', '</w>'): 1,
        ('p', 'l'): 1,
        ('l', 'e'): 1,
        ('e', 's'): 1,
        ('s', '</w>'): 1
    }

    Args:
        bpe_vocab: Dict[str, int] - "å•è¯åˆ†è¯çŠ¶æ€->é¢‘æ•°"çš„è¯å…¸

    Return:
        Dict[Tuple(str, str), int] - "bigram->é¢‘æ•°"çš„è¯å…¸
    """

    bigram_freq = dict()
    
    # TODO: å®Œæˆå‡½æ•°ä½“ï¼ˆ1åˆ†ï¼‰
    for ch, fr in bpe_vocab.items(): #éå†è¯è¡¨çš„æ¯ä¸ªè¯ã€è¯é¢‘
        word = ch.split() #å…ˆå°†æ¯ä¸ªä»¥ç©ºæ ¼éš”å¼€çš„å­—ç¬¦å­˜å…¥åˆ—è¡¨
        for i in range(1, len(word)):
            if tuple(word[i-1:i+1]) in bigram_freq: #å¯¹æ¯ä¸ªè¯è¿›è¡Œå­—æ¯ä¸¤ä¸¤ç»„åˆ
                bigram_freq[tuple(word[i - 1:i + 1])] += fr #å¡«å…¥æ–°çš„bigramè¯è¡¨ï¼ˆè¡¨ä¸­æœ‰bigramåˆ™è¯é¢‘å¢åŠ ï¼Œæ²¡æœ‰åˆ™åˆ›ç«‹æ–°çš„é”®å€¼å¯¹ï¼‰
            else:
                bigram_freq[tuple(word[i - 1:i + 1])] = fr
    return bigram_freq


def refresh_bpe_vocab_by_merging_bigram(bigram, old_bpe_vocab):
    """
    åœ¨"å•è¯åˆ†è¯çŠ¶æ€->é¢‘æ•°"çš„è¯å…¸ä¸­ï¼Œåˆå¹¶æŒ‡å®šçš„bigramï¼ˆå³å»æ‰å¯¹åº”çš„ç›¸é‚»unigramä¹‹é—´çš„ç©ºæ ¼ï¼‰ï¼Œæœ€åè¿”å›æ–°çš„è¯å…¸ï¼Œä¾‹å¦‚ï¼š
    è¾“å…¥ bigram=('i', '</w>')ï¼Œold_bpe_vocab=
    {
        'i </w>': 2,
        'a m </w>': 1,
        'h a p p y </w>': 1,
        'h a v e </w>': 1,
        'N </w>': 1,
        'a p p l e s </w>': 1
    }
    å¾—åˆ°
    {
        'i</w>': 2,
        'a m </w>': 1,
        'h a p p y </w>': 1,
        'h a v e </w>': 1,
        'N </w>': 1,
        'a p p l e s </w>': 1
    }

    Args:
        old_bpe_vocab: Dict[str, int] - åˆå§‹"å•è¯åˆ†è¯çŠ¶æ€->é¢‘æ•°"çš„è¯å…¸

    Return:
        Dict[str, int] - åˆå¹¶åçš„"å•è¯åˆ†è¯çŠ¶æ€->é¢‘æ•°"çš„è¯å…¸
    """
    
    new_bpe_vocab = dict()

    # TODO: å®Œæˆå‡½æ•°ä½“ï¼ˆ1åˆ†ï¼‰
    '''
    word = " ".join(bigram) #å°†bigramå…ƒç»„æ”¹ä¸ºä»¥ç©ºæ ¼ä¸ºé—´çš„å­—ç¬¦ä¸²
    for ch, fr in old_bpe_vocab.items(): # éå†æ—§è¯è¡¨çš„æ¯ä¸ªè¯ã€è¯é¢‘
        if word in ch: # bigramåŒ¹é…ä¸Šäº†æŸä¸ªè¯
            lft = ch.index(word) #æ‰¾åˆ°bigramåœ¨è¯ä¸­çš„èµ·å§‹ä½
            rht = lft + len(word) #æ‰¾åˆ°bigramåœ¨è¯ä¸­çš„ç»“æŸä½
            new_bpe_vocab[ch[0:lft] + "".join(bigram) + ch[rht:]] = fr #åˆå¹¶bigramï¼ˆå³å»æ‰å¯¹åº”çš„ç›¸é‚»unigramä¹‹é—´çš„ç©ºæ ¼ï¼‰
        else:
            new_bpe_vocab[ch] = fr #bigramæœªåŒ¹é…ï¼Œåˆ™ç›´æ¥å½•å…¥
    return new_bpe_vocab
    '''
    word = " ".join(bigram)  # å°†bigramå…ƒç»„æ”¹ä¸ºä»¥ç©ºæ ¼ä¸ºé—´çš„å­—ç¬¦ä¸²
    for ch, fr in old_bpe_vocab.items():  # éå†æ—§è¯è¡¨çš„æ¯ä¸ªè¯ã€è¯é¢‘
        if word in ch:  # bigramåŒ¹é…ä¸Šäº†æŸä¸ªè¯
            lft = 0  # æ‰¾åˆ°bigramåœ¨è¯ä¸­çš„èµ·å§‹ä½
            rht = lft + len(word)  # æ‰¾åˆ°bigramåœ¨è¯ä¸­çš„ç»“æŸä½
            new_ch = ''
            for i in range(ch.count(word)):
                lft1 = ch[lft:].index(word) + len(ch[0:lft])  # æ‰¾åˆ°bigramåœ¨è¯ä¸­çš„èµ·å§‹ä½
                rht = lft1 + len(word)  # æ‰¾åˆ°bigramåœ¨è¯ä¸­çš„ç»“æŸä½
                new_ch += ch[lft:lft1] + "".join(bigram)
                lft = rht
            new_ch += ch[rht:]
            new_bpe_vocab[new_ch] = fr  # åˆå¹¶bigramï¼ˆå³å»æ‰å¯¹åº”çš„ç›¸é‚»unigramä¹‹é—´çš„ç©ºæ ¼ï¼‰
        else:
            new_bpe_vocab[ch] = fr  # bigramæœªåŒ¹é…ï¼Œåˆ™ç›´æ¥å½•å…¥
    return new_bpe_vocab


def get_bpe_tokens(bpe_vocab):
    """
    æ ¹æ®"å•è¯åˆ†è¯çŠ¶æ€->é¢‘æ•°"çš„è¯å…¸ï¼Œè¿”å›æ‰€å¾—åˆ°çš„BPEåˆ†è¯åˆ—è¡¨ï¼Œå¹¶å°†è¯¥åˆ—è¡¨æŒ‰ç…§åˆ†è¯é•¿åº¦é™åºæ’åºè¿”å›ï¼Œä¾‹å¦‚ï¼š
    è¾“å…¥ bpe_vocab=
    {
        'i</w>': 2,
        'a m </w>': 1,
        'ha pp y </w>': 1,
        'ha v e </w>': 1,
        'N </w>': 1,
        'a pp l e s </w>': 1
    }
    å¾—åˆ°
    [
        ('i</w>', 2),
        ('ha', 2),
        ('pp', 2),
        ('a', 2),
        ('m', 1),
        ('</w>', 5),
        ('y', 1),
        ('v', 1),
        ('e', 2),
        ('N', 1),
        ('l', 1),
        ('s', 1)
     ]

    Args:
        bpe_vocab: Dict[str, int] - "å•è¯åˆ†è¯çŠ¶æ€->é¢‘æ•°"çš„è¯å…¸

    Return:
        List[Tuple(str, int)] - BPEåˆ†è¯å’Œå¯¹åº”é¢‘æ•°ç»„æˆçš„List
    """
    
    # TODO: å®Œæˆå‡½æ•°ä½“ï¼ˆ2åˆ†ï¼‰
    bpe_token = dict() #åˆ›å»ºå­—å…¸ä¸´æ—¶å­˜å‚¨
    for ch, fr in bpe_vocab.items():
        word = ch.split() #å…ˆå°†æ¯ä¸ªä»¥ç©ºæ ¼éš”å¼€çš„å­—ç¬¦å­˜å…¥åˆ—è¡¨
        for gram in word: #éå†å­—ç¬¦
            if gram in bpe_token:
                bpe_token[gram] += fr # å­˜å…¥å­—ç¬¦ã€å­—é¢‘ï¼ˆè¡¨ä¸­æœ‰è¯¥å­—ç¬¦åˆ™å­—é¢‘å¢åŠ ï¼Œæ²¡æœ‰åˆ™åˆ›ç«‹æ–°çš„é”®å€¼å¯¹ï¼‰
            else:
                bpe_token[gram] = fr
    bpe_tokens = []
    for ch, fr in bpe_token.items(): #å°†ç»Ÿè®¡å¥½çš„å­—å…¸è¾“å…¥åˆ—è¡¨
        bpe_tokens.append((ch, fr))
    # å°†è¯¥åˆ—è¡¨æŒ‰ç…§åˆ†è¯é•¿åº¦æ’åºè¿”å›ï¼ˆ'</w>'è®¡ä¸ºä¸€ä¸ªé•¿åº¦ï¼‰
    bpe_tokens = sorted(bpe_tokens, key=lambda pair: len(pair[0]) if '</w>' not in pair[0] else len(pair[0]) - 3)
    bpe_tokens.reverse() #æ”¹ä¸ºé™åº
    return bpe_tokens


def print_bpe_tokenize(word, bpe_tokens):
    """
    æ ¹æ®æŒ‰é•¿åº¦é™åºçš„BPEåˆ†è¯åˆ—è¡¨ï¼Œå°†æ‰€ç»™å•è¯è¿›è¡ŒBPEåˆ†è¯ï¼Œæœ€åæ‰“å°ç»“æœã€‚
    
    æ€æƒ³æ˜¯ï¼Œå¯¹äºä¸€ä¸ªå¾…BPEåˆ†è¯çš„å•è¯ï¼ŒæŒ‰ç…§é•¿åº¦é¡ºåºä»åˆ—è¡¨ä¸­å¯»æ‰¾BPEåˆ†è¯è¿›è¡Œå­ä¸²åŒ¹é…ï¼Œ
    è‹¥æˆåŠŸåŒ¹é…ï¼Œåˆ™å¯¹è¯¥å­ä¸²å·¦å³çš„å‰©ä½™éƒ¨åˆ†é€’å½’åœ°è¿›è¡Œä¸‹ä¸€è½®åŒ¹é…ï¼Œç›´åˆ°å‰©ä½™éƒ¨åˆ†é•¿åº¦ä¸º0ï¼Œ
    æˆ–è€…å‰©ä½™éƒ¨åˆ†æ— æ³•åŒ¹é…ï¼ˆè¯¥éƒ¨åˆ†æ•´ä½“ç”±"<unknown>"ä»£æ›¿ï¼‰
    
    ä¾‹1ï¼š
    è¾“å…¥ word="supermarket", bpe_tokens=[
        ("su", 20),
        ("are", 10),
        ("per", 30),
    ]
    æœ€ç»ˆæ‰“å° "su per <unknown>"

    ä¾‹2ï¼š
    è¾“å…¥ word="shanghai", bpe_tokens=[
        ("hai", 1),
        ("sh", 1),
        ("an", 1),
        ("</w>", 1),
        ("g", 1)
    ]
    æœ€ç»ˆæ‰“å° "sh an g hai </w>"

    Args:
        word: str - å¾…åˆ†è¯çš„å•è¯str
        bpe_tokens: List[Tuple(str, int)] - BPEåˆ†è¯å’Œå¯¹åº”é¢‘æ•°ç»„æˆçš„List
    """
    
    # TODO: è¯·å°è¯•ä½¿ç”¨é€’å½’å‡½æ•°å®šä¹‰è¯¥åˆ†è¯è¿‡ç¨‹ï¼ˆ2åˆ†ï¼‰
    def bpe_tokenize(sub_word):
        if len(sub_word) == 0: #é€’å½’ç»“æŸæ¡ä»¶ï¼šå‰©ä½™éƒ¨åˆ†é•¿åº¦ä¸º0
            return ""
        for i, pair in enumerate(bpe_tokens): #éå†bpråˆ†è¯åˆ—è¡¨
            if pair[0] in sub_word: #æˆåŠŸåŒ¹é…
                lft = sub_word.index(pair[0]) #æ‰¾å‡ºbpeåˆ†è¯åœ¨å•è¯ä¸­çš„èµ·å§‹ä½
                rht = lft + len(pair[0]) #æ‰¾å‡ºbpeåˆ†è¯åœ¨å•è¯ä¸­çš„ç»“æŸä½
                return bpe_tokenize(sub_word[0:lft]) + pair[0] + " " + bpe_tokenize(sub_word[rht:]) #å¯¹è¯¥å­ä¸²å·¦å³çš„å‰©ä½™éƒ¨åˆ†é€’å½’åœ°è¿›è¡Œä¸‹ä¸€è½®åŒ¹é…
            else:
                if i == len(bpe_tokens) - 1: #é€’å½’ç»“æŸæ¡ä»¶ï¼šå‰©ä½™éƒ¨åˆ†æ— æ³•åŒ¹é…ï¼ˆè¯¥éƒ¨åˆ†æ•´ä½“ç”±"<unknown>"ä»£æ›¿ï¼‰
                    return "<unknown>" + " "
        return ""

    res = bpe_tokenize(word+"</w>")
    print(res)


# å¼€å§‹è¯»å–æ•°æ®é›†å¹¶è®­ç»ƒBPEåˆ†è¯å™¨
with open("data/news.2007.en.shuffled.deduped.train", encoding="utf-8") as f:
    training_corpus = list(map(lambda l: l.strip(), f.readlines()[:1000]))

print("Loaded training corpus.")

training_iter_num = 300

training_bpe_vocab = build_bpe_vocab(training_corpus)
for i in range(training_iter_num):
    # TODO: å®Œæˆè®­ç»ƒå¾ªç¯å†…çš„ä»£ç é€»è¾‘ï¼ˆ2åˆ†ï¼‰
    bigram_freq = get_bigram_freq(training_bpe_vocab) #åˆ›å»ºbigramè¯è¡¨
    max_key = max(bigram_freq, key=bigram_freq.get) #æ‰¾åˆ°bigramè¯è¡¨ä¸­æœ€å¸¸è§çš„ä¸€ä¸ªbigram
    training_bpe_vocab = refresh_bpe_vocab_by_merging_bigram(max_key, training_bpe_vocab) #å°†æœ€å¸¸è§çš„bigramæåˆæˆæ–°çš„tokenï¼Œæ„æˆæ–°è¯è¡¨

training_bpe_tokens = get_bpe_tokens(training_bpe_vocab)


# æµ‹è¯•BPEåˆ†è¯å™¨çš„åˆ†è¯æ•ˆæœ
test_word = "naturallanguageprocessing"

print("naturallanguageprocessing çš„åˆ†è¯ç»“æœä¸ºï¼š")
print_bpe_tokenize(test_word, training_bpe_tokens)
# result: n atur al lan gu age pro ce s sing</w>